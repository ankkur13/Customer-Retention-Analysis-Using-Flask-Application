{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from boruta import BorutaPy\n",
    "import pickle\n",
    "import boto\n",
    "from boto.s3.key import Key\n",
    "from boto.s3.connection import Location\n",
    "import os, sys, time\n",
    "\n",
    "DATA_DIRECTORY = 'trainingDataFromS3'\n",
    "UPLOAD_DIRECTORY = 'uploadDataToS3'\n",
    "BUCKET_NAME = 'ads-final-project'\n",
    "\n",
    "def functions_ignitor(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY):\n",
    "    create_directory(DATA_DIRECTORY)\n",
    "    create_directory(UPLOAD_DIRECTORY)\n",
    "    print('MAIN FUNCTION TRIGGERED')\n",
    "    \n",
    "    downloadFromS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    print('DOWNLOADED DATA DUMP FROM S3')\n",
    "    \n",
    "    X_all, y_all, df_dummies, upload_indexFilePath = data_transformation()\n",
    "    print('DATA TRANSFORMATION COMPLETED')\n",
    "    \n",
    "    X_selected, y_selected, upload_featuredIndexFilePath = feature_engineering(X_all, y_all, df_dummies)\n",
    "    print('FEATURE ENGINEERING COMPLETED')\n",
    "    \n",
    "    trained_models_with_rank, upload_metricsFilePath = model_training(X_selected, y_selected)\n",
    "    print('MODEL TRAINING COMPLETED')    \n",
    "    \n",
    "    upload_modelFilePath = pickle_trained_model(trained_models_with_rank)\n",
    "    print('PICKLED ALL MODELS')    \n",
    "    \n",
    "    uploadToS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, upload_indexFilePath)\n",
    "    print('UPLOADED PICKLE INDEX TO S3')\n",
    "     \n",
    "    uploadToS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, upload_featuredIndexFilePath)\n",
    "    print('UPLOADED PICKLE FEATURED INDEX TO S3')\n",
    "    \n",
    "    uploadToS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, upload_metricsFilePath)\n",
    "    print('UPLOADED METRICS CSV TO S3')\n",
    "    \n",
    "    uploadToS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, upload_modelFilePath)\n",
    "    print('UPLOADED PICKLE MODEL TO S3')\n",
    "    \n",
    "def create_directory(directory_name):\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "\n",
    "def downloadFromS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY):\n",
    "    conn = boto.connect_s3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    bucket = conn.get_bucket(BUCKET_NAME)\n",
    "    bucket_list = bucket.list()\n",
    "    for l in bucket_list:\n",
    "        print('l: ', l)\n",
    "        keyString = str(l.key)\n",
    "        print('keyString: ', keyString)\n",
    "        l.get_contents_to_filename(keyString)\n",
    "\n",
    "def uploadToS3(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, upload_filePath, destinationPath = ''):\n",
    "    conn = boto.connect_s3(AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY)\n",
    "    bucket = conn.create_bucket(BUCKET_NAME,location=boto.s3.connection.Location.DEFAULT)\n",
    "    print ('Uploading '+upload_filePath+' to Amazon S3 bucket '+BUCKET_NAME)\n",
    "    def percent_cb(complete, total):\n",
    "        sys.stdout.write('.')\n",
    "        sys.stdout.flush()   \n",
    "    k = Key(bucket)\n",
    "    k.key = destinationPath+\"/\"+upload_filePath\n",
    "    k.set_contents_from_filename(upload_filePath, cb = percent_cb, num_cb = 10)\n",
    "    print('Uploaded')\n",
    "\n",
    "def data_transformation():\n",
    "    df = pd.read_csv(DATA_DIRECTORY+'/churn_train.csv')\n",
    "    # Pickle all_features for Form Uploads\n",
    "    df_to_index_pickle = df.drop([\"Churn\"], axis=1)\n",
    "    upload_indexFilePath = pickle_df_index(df_to_index_pickle, 'index_dict.pkl')\n",
    "    # Replacing empty spaces with Null values\n",
    "    df = df.replace(r'^\\s+$', np.nan, regex=True)\n",
    "    # Dropping NA values\n",
    "    df = df.dropna()\n",
    "    # Deleting the custumerID column\n",
    "    del df[\"customerID\"]\n",
    "    # Removing TotalCharges variable from the data\n",
    "    del df[\"TotalCharges\"]\n",
    "    #Converting SeniorCitizen variable into categorical and mapping values of Yes & No to 1 & 0 respectively\n",
    "    df['SeniorCitizen'] = df.SeniorCitizen.map({0:'No', 1:'Yes'})\n",
    "    all_columns_list = df.columns.tolist()\n",
    "    numerical_columns_list = ['tenure','MonthlyCharges']\n",
    "    categorical_columns_list = [e for e in all_columns_list if e not in numerical_columns_list]\n",
    "    for index in categorical_columns_list:\n",
    "        df[index] = pd.Categorical(df[index])\n",
    "    for index in numerical_columns_list:\n",
    "        df[index] = pd.to_numeric(df[index])\n",
    "    # Splitting data according to datatypes\n",
    "    num = ['float64', 'int64']\n",
    "    num_df = df.select_dtypes(include=num)\n",
    "    obj_df = df.select_dtypes(exclude=num)\n",
    "    # Creating bins for numerical variables for extensive prediction of churn\n",
    "    tenure_bins = pd.cut(num_df[\"tenure\"], bins=[0,20,60,80], labels=['low','medium','high'])\n",
    "    MonthlyCharges_bins = pd.cut(num_df[\"MonthlyCharges\"], bins=[0,35,60,130], labels=['low','medium','high'])\n",
    "    # Saving numeric variable bins into a dataframe\n",
    "    bins = pd.DataFrame([tenure_bins, MonthlyCharges_bins]).T\n",
    "    # Concatenate bins with object variables\n",
    "    transformed_df = pd.concat([bins,obj_df],axis=1)\n",
    "    dummy_columns = [e for e in transformed_df.columns if e != 'Churn']\n",
    "    # Creating dataframe of dummy variables\n",
    "    df_dummies = pd.get_dummies(data=transformed_df, columns=dummy_columns)\n",
    "    df_dummies_features = df_dummies.drop([\"Churn\"], axis=1).columns\n",
    "    X_all = df_dummies[df_dummies_features]\n",
    "    y_all = df_dummies[\"Churn\"]\n",
    "    return X_all, y_all, df_dummies, upload_indexFilePath\n",
    "\n",
    "def feature_engineering(X_all, y_all, df_dummies):\n",
    "    # Change X and y to its values\n",
    "    X_boruta = X_all.values\n",
    "    y_boruta = y_all.values\n",
    "    # Define random forest classifier, with utilising all cores and sampling in proportion to y labels\n",
    "    rfc = RandomForestClassifier(n_jobs = -1)\n",
    "    # Define Boruta feature selection method\n",
    "    feature_selector = BorutaPy(rfc, n_estimators='auto', random_state=1)\n",
    "    # Find all relevant features\n",
    "    feature_selector.fit(X_boruta, y_boruta)\n",
    "    #Transposing dataframe for ranking\n",
    "    df_features_rank = df_dummies.drop(['Churn'],axis=1).T\n",
    "    # Check ranking of features\n",
    "    df_features_rank['Boruta_Rank'] = feature_selector.ranking_\n",
    "    # Adding a variable 'Feature' in the dataframe\n",
    "    df_features_rank['Feature']=  df_features_rank.index\n",
    "    # Sort the dataframe as per Rank\n",
    "    df_features_rank = df_features_rank.sort_values('Boruta_Rank')\n",
    "    # Exctracting only top 2 ranked features\n",
    "    df_top2_ranked_feature = df_features_rank.loc[df_features_rank['Boruta_Rank'].isin([1,2])]\n",
    "    # Selecting important featutres\n",
    "    selected_features = df_top2_ranked_feature.index\n",
    "    X_selected = df_dummies[selected_features]\n",
    "    y_selected = df_dummies[\"Churn\"]\n",
    "    # Pickle the selected features for Form Uploads\n",
    "    upload_featuredIndexFilePath = pickle_df_index(X_selected, 'featured_index_dict.pkl')    \n",
    "    return X_selected, y_selected, upload_featuredIndexFilePath\n",
    "\n",
    "def model_training(X_selected, y_selected):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X_selected, y_selected, test_size=0.20, random_state=7)\n",
    "    # Make predictions on test dataset\n",
    "    models = []\n",
    "    accuracy_list = []\n",
    "    trained_models = {}\n",
    "    models.append(('LogisticRegression', LogisticRegression()))\n",
    "    models.append(('KNeighborsClassifier', KNeighborsClassifier()))\n",
    "    models.append(('DecisionTreeClassifier', DecisionTreeClassifier()))\n",
    "    models.append(('RandomForestClassifier', RandomForestClassifier()))\n",
    "    for name, model in models:\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[name] = model\n",
    "        predictions = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, predictions)\n",
    "        accuracy_list.append((name,acc))\n",
    "    #Creating a dataframe for the models metrics\n",
    "    models_metrics = pd.DataFrame(accuracy_list, columns=[\"Model\", \"Accuracy\"]) \n",
    "    models_metrics['Model_Rank'] = models_metrics['Accuracy'].rank(ascending=False, method='first')\n",
    "    # Store the result into csv\n",
    "    upload_metricsFilePath = UPLOAD_DIRECTORY+'/metrics_score.csv'\n",
    "    models_metrics.to_csv(upload_metricsFilePath, index=False)\n",
    "    # Compiling all the models in single dictionary\n",
    "    rank_dict = pd.Series(models_metrics.Model_Rank.values, index=models_metrics.Model.values).to_dict()\n",
    "    trained_models_with_rank = {}\n",
    "    for key, value in rank_dict.items():\n",
    "        trained_models_with_rank[rank_dict[key]] = [value1 for key1, value1 in trained_models.items() if key == key1]\n",
    "        trained_models_with_rank[rank_dict[key]].append(key)\n",
    "    return trained_models_with_rank, upload_metricsFilePath\n",
    "\n",
    "def pickle_trained_model(trained_models_with_rank):\n",
    "    upload_modelFilePath = UPLOAD_DIRECTORY+'/pickled_models.pkl'\n",
    "    with open(upload_modelFilePath, \"wb\") as fp:\n",
    "        pickle.dump(trained_models_with_rank, fp, protocol=2)\n",
    "    return upload_modelFilePath\n",
    "\n",
    "def pickle_df_index(df, filename):\n",
    "    # Pickle index for Form Uploads\n",
    "    index_dict = dict(zip(df.columns,range(df.shape[1])))\n",
    "    upload_indexFilePath = UPLOAD_DIRECTORY+'/'+filename\n",
    "    with open(upload_indexFilePath, \"wb\") as fp:\n",
    "        pickle.dump(index_dict, fp, protocol=2)\n",
    "    return upload_indexFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_ACCESS_KEY_ID = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_SECRET_ACCESS_KEY = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAIN FUNCTION TRIGGERED\n",
      "l:  <Key: ads-final-project-data-dump,churn_test.csv>\n",
      "keyString:  churn_test.csv\n",
      "l:  <Key: ads-final-project-data-dump,churn_train.csv>\n",
      "keyString:  churn_train.csv\n",
      "DOWNLOADED DATA DUMP FROM S3\n",
      "DATA TRANSFORMATION COMPLETED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n",
      "C:\\Users\\eklav\\Anaconda3\\lib\\site-packages\\boruta\\boruta_py.py:418: RuntimeWarning: invalid value encountered in greater\n",
      "  hits = np.where(cur_imp[0] > imp_sha_max)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING COMPLETED\n",
      "MODEL TRAINING COMPLETED\n",
      "PICKLED ALL MODELS\n",
      "Uploading uploadDataToS3/index_dict.pkl to Amazon S3 bucket ads-final-project\n",
      "..Uploaded\n",
      "UPLOADED PICKLE INDEX TO S3\n",
      "Uploading uploadDataToS3/featured_index_dict.pkl to Amazon S3 bucket ads-final-project\n",
      "..Uploaded\n",
      "UPLOADED PICKLE FEATURED INDEX TO S3\n",
      "Uploading uploadDataToS3/metrics_score.csv to Amazon S3 bucket ads-final-project\n",
      "..Uploaded\n",
      "UPLOADED METRICS CSV TO S3\n",
      "Uploading uploadDataToS3/pickled_models.pkl to Amazon S3 bucket ads-final-project\n",
      "..........Uploaded\n",
      "UPLOADED PICKLE MODEL TO S3\n",
      "PROGRAM EXECUTED SUCCESSFULLY\n"
     ]
    }
   ],
   "source": [
    "functions_ignitor(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "print('PROGRAM EXECUTED SUCCESSFULLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
